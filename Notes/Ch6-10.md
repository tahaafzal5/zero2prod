- [Ch 6 - Reject Invalid Subscribers #1](#ch-6---reject-invalid-subscribers-1)
  - [Requirements](#requirements)
    - [Domain Constraints](#domain-constraints)
    - [Security Constraints](#security-constraints)
  - [First Implementation](#first-implementation)
  - [Validation Is A Leaky Cauldron](#validation-is-a-leaky-cauldron)
  - [Type-Driven Development](#type-driven-development)
  - [Ownership Meets Invariants](#ownership-meets-invariants)
    - [AsRef](#asref)
  - [Error As Values - Result](#error-as-values---result)
    - [Converting `parse` To Return `Result`](#converting-parse-to-return-result)
  - [Insightful Assertion Errors: `claims`](#insightful-assertion-errors-claims)
  - [Unit Tests](#unit-tests)
  - [Handling A `Result`](#handling-a-result)
    - [The `?` Operator](#the--operator)
    - [400 Bad Request](#400-bad-request)
  - [The Email Format](#the-email-format)
  - [The `SubscriberEmail` Type](#the-subscriberemail-type)
    - [Breaking The Domain Sub-Module](#breaking-the-domain-sub-module)
    - [Skeleton Of A New Type](#skeleton-of-a-new-type)
  - [Property-based Testing](#property-based-testing)
    - [How To Generate Random Test Data With `fake`](#how-to-generate-random-test-data-with-fake)
    - [Getting Started with `quickcheck`](#getting-started-with-quickcheck)
    - [Implementing the `Arbitrary` Trait](#implementing-the-arbitrary-trait)
  - [Payload Validation](#payload-validation)
    - [Refactoring With `TryForm`](#refactoring-with-tryform)
- [Ch 7 - Reject Invalid Subscribers #2](#ch-7---reject-invalid-subscribers-2)
  - [Confirmation Emails](#confirmation-emails)
    - [Subscriber Consent](#subscriber-consent)
    - [The Confirmation User Journey](#the-confirmation-user-journey)
    - [The Implementation Strategy](#the-implementation-strategy)
  - [`EmailClient`, Our Email Delivery Component](#emailclient-our-email-delivery-component)
    - [How To Send An Email](#how-to-send-an-email)
      - [Choosing An Email API](#choosing-an-email-api)
      - [The Email Client Interface](#the-email-client-interface)
    - [How To Write A REST Client Using reqwest](#how-to-write-a-rest-client-using-reqwest)
      - [`reqwest::Client`](#reqwestclient)
      - [Connection Pooling](#connection-pooling)
      - [How To Reuse The Same reqwest::Client In `actix-web`](#how-to-reuse-the-same-reqwestclient-in-actix-web)
      - [Configuring Our EmailClient](#configuring-our-emailclient)
    - [How To Test A REST Client](#how-to-test-a-rest-client)
      - [`wiremock::MockServer`](#wiremockmockserver)
      - [`wiremock::Mock`](#wiremockmock)
      - [The Intent Of A Test Should Be Clear](#the-intent-of-a-test-should-be-clear)
      - [Mock expectations](#mock-expectations)
    - [First Sketch Of `EmailClient::send_email`](#first-sketch-of-emailclientsend_email)
      - [`reqwest::Client::post`](#reqwestclientpost)
      - [JSON body](#json-body)
    - [Tightening Our Happy Path Test](#tightening-our-happy-path-test)
        - [Headers, Path And Method](#headers-path-and-method)
        - [Body](#body)
      - [Refactoring: Avoid Unnecessary Memory Allocations](#refactoring-avoid-unnecessary-memory-allocations)
    - [Dealing With Failures](#dealing-with-failures)
      - [Error Status Codes](#error-status-codes)
      - [Timeouts](#timeouts)
      - [Refactoring: Test Helpers](#refactoring-test-helpers)
      - [Refactoring: Fail fast](#refactoring-fail-fast)
  - [Skeleton And Principles For A Maintainable Test Suite](#skeleton-and-principles-for-a-maintainable-test-suite)
    - [Why Do We Write Tests?](#why-do-we-write-tests)
    - [Why Don't We Write Tests?](#why-dont-we-write-tests)
    - [Test Code Is Still Code](#test-code-is-still-code)
    - [Our Test Suite](#our-test-suite)
    - [Test Discovery](#test-discovery)
    - [One Test File, One Crate](#one-test-file-one-crate)
    - [Sharing Test Helpers](#sharing-test-helpers)
    - [Sharing Startup Logic](#sharing-startup-logic)
      - [Extracting Our Startup Code](#extracting-our-startup-code)
    - [Build An API Client](#build-an-api-client)
  - [Refocus](#refocus)
  - [Zero Downtime Deployments](#zero-downtime-deployments)
    - [Reliability](#reliability)
    - [Deployment Strategies](#deployment-strategies)
      - [Naive Deployment](#naive-deployment)
      - [Load Balancers](#load-balancers)
        - [Horizontal Scaling](#horizontal-scaling)
        - [Health Checks](#health-checks)
      - [Rolling Update Deployments](#rolling-update-deployments)
      - [Digital Ocean App Platform](#digital-ocean-app-platform)
  - [Database Migrations](#database-migrations)
    - [Deployments And Migrations](#deployments-and-migrations)
    - [Multi-step Migrations](#multi-step-migrations)
    - [A New Mandatory Column](#a-new-mandatory-column)
      - [Step 1: Add As Optional](#step-1-add-as-optional)
      - [Step 2: Start Using The New Column](#step-2-start-using-the-new-column)
      - [Step 3: Backfill And Mark As `NOT NULL`](#step-3-backfill-and-mark-as-not-null)
    - [A New Table](#a-new-table)
  - [Sending A Confirmation Email](#sending-a-confirmation-email)
    - [A Static Email](#a-static-email)
      - [Red Test](#red-test)
      - [Green Test](#green-test)
    - [A Static Confirmation Link](#a-static-confirmation-link)
      - [Red Test](#red-test-1)
      - [Green Test](#green-test-1)
    - [Pending Confirmation](#pending-confirmation)
      - [Red Test](#red-test-2)
      - [Green Test](#green-test-2)
    - [Skeleton of `GET /subscriptions/confirm`](#skeleton-of-get-subscriptionsconfirm)
      - [Red Test](#red-test-3)
      - [Green Test](#green-test-3)
    - [Connecting The Dots](#connecting-the-dots)
      - [Red Test](#red-test-4)
      - [Green Test](#green-test-4)
      - [Refactor](#refactor)
    - [Subscription Tokens](#subscription-tokens)
      - [Red Test](#red-test-5)
      - [Green Test](#green-test-5)
  - [Database Transactions](#database-transactions)
    - [All Or Nothing](#all-or-nothing)
    - [Transactions In Postgres](#transactions-in-postgres)
    - [Transactions In Sqlx](#transactions-in-sqlx)
- [Ch 8 - Error Handling](#ch-8---error-handling)
  - [What Is The Purpose Of Errors?](#what-is-the-purpose-of-errors)
    - [Internal Errors](#internal-errors)
      - [Enable The Caller To React](#enable-the-caller-to-react)
      - [Help An Operator To Troubleshoot](#help-an-operator-to-troubleshoot)
    - [Errors At The Edge](#errors-at-the-edge)
      - [Help A User To Troubleshoot](#help-a-user-to-troubleshoot)
  - [Error Reporting For Operators](#error-reporting-for-operators)
    - [Keeping Track Of The Error Root Cause](#keeping-track-of-the-error-root-cause)
    - [The `Error` Trait](#the-error-trait)
      - [Trait Objects](#trait-objects)
      - [`Error::source`](#errorsource)
  - [Errors For Control Flow](#errors-for-control-flow)
    - [Layering](#layering)
    - [Modeling Errors as Enums](#modeling-errors-as-enums)
    - [The Error Type Is Not Enough](#the-error-type-is-not-enough)
    - [Removing The Boilerplate With `thiserror`](#removing-the-boilerplate-with-thiserror)
  - [Avoid "Ball Of Mud" Error Enums](#avoid-ball-of-mud-error-enums)
    - [Using `anyhow` As Opaque Error Type](#using-anyhow-as-opaque-error-type)
    - [`anyhow` Or `thiserror`](#anyhow-or-thiserror)
  - [Who Should Log Errors?](#who-should-log-errors)
- [Ch 9 - Naive Newsletter Delivery](#ch-9---naive-newsletter-delivery)
  - [User Stories Are Not Set In Stone](#user-stories-are-not-set-in-stone)
  - [Do Not Spam Unconfirmed Subscribers](#do-not-spam-unconfirmed-subscribers)
    - [Set Up State Using The Public API](#set-up-state-using-the-public-api)
    - [Scoped Mocks](#scoped-mocks)
    - [Green Test](#green-test-6)
  - [All Confirmed Subscribers Receive New Issues](#all-confirmed-subscribers-receive-new-issues)
    - [Composing Test Helpers](#composing-test-helpers)
  - [Implementation Strategy](#implementation-strategy)
  - [Body Schema](#body-schema)
    - [Test Invalid Inputs](#test-invalid-inputs)
  - [Fetch Confirmed Subscribers List](#fetch-confirmed-subscribers-list)
  - [Send Newsletter Email](#send-newsletter-email)
    - [`context` Vs `with_context`](#context-vs-with_context)
  - [Validation of Stored Data](#validation-of-stored-data)
    - [Responsibility Boundaries](#responsibility-boundaries)
  - [Limitations Of The Naive Approach](#limitations-of-the-naive-approach)
- [Ch 10 - Securing Our API](#ch-10---securing-our-api)
  - [Authentication](#authentication)
    - [Drawbacks](#drawbacks)
      - [Something They Know](#something-they-know)
      - [Something They Have](#something-they-have)
      - [Something They Are](#something-they-are)
    - [Multi-factor Authentication](#multi-factor-authentication)
  - [Password-based Authentication](#password-based-authentication)
      - [Extracting Credentials](#extracting-credentials)
    - [Password Verification - Naive Approach](#password-verification---naive-approach)
    - [Password Storage](#password-storage)
      - [No Need To Store Raw Passwords](#no-need-to-store-raw-passwords)

# Ch 6 - Reject Invalid Subscribers #1
* Our input validation for `/POST` is limited: we just ensure that both the `name` and the `email` fields are provided, even if they are empty.

## Requirements
### Domain Constraints
* For `name`, we will just require it to be non-empty.

### Security Constraints
* Forms and user inputs are a primary attack target - if they are not properly sanitized, they might allow an attacker to mess with our database (SQL injection), execute code on our servers, crash our service, etc.
* We are building an email newsletter, which leads us to focus on:
  1. denial of service - e.g. trying to take our service down to prevent other people from signing up. A common threat for basically any online service
  2. data theft -e.g. steal a huge list of email addresses
  3. phishing -e.g. use our service to send what looks like a legitimate email to a victim to trick them into clicking on some links or perform other actions
* We will be:
  * Enforcing a maximum length. We are using `TEXT` as type for our email in Postgres, which is unbounded until disk storage starts to run out. 256 characters should be enough for the greatest majority of our users' name - if not, we will ask them to enter a nickname
  * Reject names containing troublesome characters. `/()"<>\{}` as they are not common in names. Forbidding them raises the complexity bar for SQL injection and phishing attempts.

## First Implementation
* We could add a function `is_valid_name()` that we call before we call `insert_subscriber`.
* That function would trim any whitespace in the name, ensure it isn't too long, isn't non-empty, and doesn't contain forbidden characters, but such an implementation would be a false sense of security.

## Validation Is A Leaky Cauldron
* Just by looking at the type `FormData`, `insert_subscriber` cannot assume that `form.name` will be non-empty. We would have to shift from a *local* (`insert_subscriber` function) approach to a *global* approach (the entire codebase) to ensure something ensured the name is valid.
* Every other function that uses `form.name` would need to do the same validation (and it could be missed during refactoring etc) and such implementations could result in input checks in multiple places -- also bad. This approach does not scale.
* We need is a parsing function - a routine that accepts unstructured input and, if a set of conditions holds, returns us a more structured output, an output that structurally guarantees that the invariants we care about hold from that point onwards. We can do this using **types**!

## Type-Driven Development
* We will add a new type `SubscriberName` (a tuple struct with a single unnamed field of type `String`) in a new module `Domain` to achieve what we want.
* Since the function `parse` is the only way to construct a `SubscriberName`, we have ensured that `SubsriberName` will never violate our constraints.
* Type-driven development is a powerful approach to encode the constraints of a domain we are trying to model inside the type system, leaning on the compiler to make sure they are enforced.

## Ownership Meets Invariants
* Since `SubsriberName` itself doesn't expose the internal `String`, and we don't want to, we will add a `inner_ref` method to give a shared reference to the inner string.

### AsRef
* While our `inner_ref` method gets the job done, Rust’s standard library exposes a trait that is designed exactly for this type of usage - `AsRef`.
* So, we will replace `inner_ref` with `as_ref`.

## Error As Values - Result
* Rust’s panics are not equivalent to exceptions in languages such as Python, C# or Java.
* Although Rust provides a few utilities to catch (some) panics, it is **not** the recommended approach.
* `Result` is used as the return type for fallible operations: if the operation succeeds, `Ok(T)` is returned; if it fails, you get `Err(E)`.
  
### Converting `parse` To Return `Result`
* We will now return a `Result<SubscriberName, String>` instead of just `SubscriberName`.

## Insightful Assertion Errors: `claims`
* We will be using the `claims` crate to get more informative error messages when a test fails when using assertions and then we can use `claims::assert_ok!()`.

## Unit Tests
* We will add some tests to the `domain` module.
* `claims` needs our type to implement the `Debug` trait to provide those nice error messages. So we will add a `#[derive(Debug)]` attribute on top of `SubscriberName`

## Handling A `Result`
* Instead of panicking when an invalid name is passed in, we want to return a "400 Bad Request".

### The `?` Operator
* `?` is syntactic sugar to reduce the amount of visual noise when you are working with fallible functions and you want to “bubble up” failures.

### 400 Bad Request
* We will return a 400 Bad Request when from `subscribe` if a bad name is used.

## The Email Format
* We will use the `validator` crate to validate emails.

## The `SubscriberEmail` Type
* We will follow the same strategy we used for `name` validation - encode our invariant (“this string represents a valid email”) in a new `SubscriberEmail` type.

### Breaking The Domain Sub-Module
* We will break down the `domain` module into `new_subscriber`, `subscriber_email`, and `subcriber_name` for better organization.
  
### Skeleton Of A New Type
* `SubscriberEmail` will also be a a tuple struct with a single unnamed field of type `String`.
* We will also implement `AsRef` and let the `validator` do all the validation for the email to be accurate.

## Property-based Testing
* We could use another approach to test our parsing logic: instead of verifying that a certain set of inputs is correctly parsed, we could build a random generator that produces valid values and check that our parser does not reject them.
* This approach is called property-based testing.
* Property-based testing significantly increases the range of inputs that we are validating, and therefore our confidence in the correctness of our code, but it does not prove that our parser is correct.

### How To Generate Random Test Data With `fake`
* `fake` provides generation logic for both primitive data types (integers, floats, strings) and higher-level objects (IP addresses, country codes, email, etc).
* But we would have to run our test suite multiple times to make sure we catch every edge case, or we could have a `for` loop to test.

### Getting Started with `quickcheck`
* There are test crates available for property-based testing, 2 of them are: `quickcheck` and `proptest`, but we will use `quickcheck`.
* `quickcheck` calls our test function in a loop with a configurable number of iterations (100 by default): on every iteration, it generates a new test case and checks the value that function returned.
* If our function fails, it tries to shrink the generated input to the smallest possible failing example to help us debug what went wrong.
* Unfortunately, if we ask for a `String` type as input we are going to get all sorts of garbage which will fail validation. How to get around this?

### Implementing the `Arbitrary` Trait
* How does `quickcheck` know what to generate to test?
* Everything is built on top of `quickcheck`'s `Arbitrary` trait that has 2 methods:
  1. `arbitrary`: given a source of randomness (`g`) it returns an instance of the type
  2. `shrink`: returns a sequence of progressively "smaller" instances of the type to help `quickcheck` find the smallest possible failure case.

* We need to create our own type, `ValidEmailFixture`, and implement `Arbitrary` for it.
* Looking at `Arbitrary`’s trait definition, `shrink` is optional: there is a default implementation (using `empty_shrinker`) which results in `quickcheck` outputting the first failure encountered, without trying to make it any smaller or nicer. So, we only need to provide an implementation of `Arbitrary::arbitrary` for our `ValidEmailFixture`.
* In `Arbitrary::arbitrary`, we get `g` as an input, an argument of type `G`.
* `G` is constraint by a trait bound, `G: quickcheck::Gen` therefore it must implement the `Gen` trait in `quickcheck`.
* And anything that implements `Gen` must also implement `RngCore` trait from the `rand-core` crate.
* We can add a `dbg!(&valid_email.0)` and run tests like `cargo t valid_emails -- --nocapture` to see all the valid emails generated.

## Payload Validation
* We can now make the changes needed in our application to use `SubsriberEmail` and all integration tests would pass.

### Refactoring With `TryForm`
* We can extract the logic to parse `name` and `email` for `NewSubscriber` into a function to get a better separation of concerns.
* We will implement `TryForm` on `NewSubscriber` to explicitly show our intent that we are convert a `FormData` into `NewSubscriber`.
* When we implement `try_form`, we automatically get the corresponding `try_into` for free that we can call.

# Ch 7 - Reject Invalid Subscribers #2

## Confirmation Emails
* Now that our emails are syntactically correct, we need to make sure they exist and reachable.

### Subscriber Consent
* We will confirm emails by sending confirmation emails and this will also tell us about the subscriber's explicit consent before we send our first newsletter.

### The Confirmation User Journey
* The user will receive an email with a confirmation link.
* Once they click on it, we will send a 200 OK to the browser.
* From that point onwards, they will receive all newsletter issues in their inbox.

### The Implementation Strategy
* There is a lot to do here, so we will split the work in three conceptual chunks:
  1. write a module to send an email
  2. adapt the logic of our existing `POST /subscriptions` request handler to match the new specification
  3. write a `GET /subscriptions/confirm` request handler from scratch

## `EmailClient`, Our Email Delivery Component

### How To Send An Email
* SMTP (Simple Mail Transfer Protocol) does for emails what HTTP does for web pages: it is an application-level protocol that ensures that different implementations of email servers and clients can understand each other and exchange messages.
* Since building our own private email server would take a long time, we will be writing a REST client for our email API provider.

#### Choosing An Email API
* We will be using Postmark.

#### The Email Client Interface
* There are two approaches when it comes to a new piece of functionality:
  1. bottom-up, starting from the implementation details and working your way up, or
  2. top-down, by designing the interface first and then figuring out how the implementation is going to work.
* We will go for the second route.

* Our `EmailClient` will be using the same sender email address to send all the emails.
* We would need the receiver's email address, the subject line, and the body of the email.
* We will be sending both HTML and a plain text version of the email content to be safe.
* Some email clients are not able to render HTML and some users explicitly disable HTML emails.
* Our `send_email` function would be asynchronous since we will be talking to a remote server.

### How To Write A REST Client Using reqwest 
* To talk to a REST API, we need an HTTP client and we will choose `reqwest` because:
  1. It has been extensively battle-tested (~8.5 million downloads)
  2. It offers a primarily asynchronous interface, with the option to enable a synchronous one via the `blocking` feature flag
  3. It relies on `tokio` as its asynchronous executor something we are already using due to `actix-web`
  4. It does not depend on any system library if you choose to use `rustls` to back the TLS implementation (`rustls-tls` feature flag instead of `default-tls`), making it extremely portable
* We are also using `reqwest` already but we will move it to a runtime dependency from a development dependency.

#### `reqwest::Client`
* `reqwest::Client` exposes all the methods we need to perform requests against a REST API.

#### Connection Pooling
* Before executing an HTTP request against an API hosted on a remote server we need to establish a connection.
* Connecting is an expensive operation, especially if using HTTPS: creating a brand-new connection every time we want to make a request can impact the performance of our application and might lead to *socket exhaustion* under load.
* To avoiding re-establishing a connection from scratch, most HTTP clients offer connection pooling: after the first request to a remote server has been completed, they will keep the connection open (for a certain amount of time) and re-use it if we need to send another request to the same server.
* `reqwest` does the same: every time a `Client` instance is created `reqwest` initializes a connection pool under the hood.
* To use this connection pool we need to reuse the same `Client` across multiple requests.
* **Note**: `Client::clone` does not create a new connection pool - we just clone a pointer to the underlying pool.

#### How To Reuse The Same reqwest::Client In `actix-web`
* To re-use the same HTTP client across multiple requests in `actix-web` we need to store a copy of it in the application context - we will then be able to retrieve a reference to `Client` in our request handlers using an extractor (e.g. `actix_web::web::Data`) similar to how we did it for the `HttpServer`.
* We have 2 options:
  1. derive the `Clone` trait for `EmailClient` build an instance of it once and then pass a clone to `app_data` every time we need to build an `App`:
  2. wrap `EmailClient` in `actix_web::web::Data` (an `Arc` pointer) and pass a pointer to `app_data` every time we need to build an App - like we are doing with `PgPool`.
* If `EmailClient` were just a wrapper around a `Client` instance, the first option would be preferable - we avoid wrapping the connection pool twice with `Arc`.
* But, `EmailClient` has two data fields attached (`base_url` and `sender`).
* The first implementation allocates new memory to hold a copy of that data every time an `App` instance is created, while the second shares it among all `App` instances. That’s why we will be using the second strategy.

#### Configuring Our EmailClient
* We will add the `base_url` and `sender_email` to our yaml configuration files and ensure we read them to initialize `EmailClient`.

### How To Test A REST Client
* The main purpose of `EmailClient::send_email` is to perform an HTTP call: how do we know if it happened? How do we check that the body and the headers were populated as we expected?
* We need to intercept that HTTP request so we will spin up a mock server using `wiremock`.

* We can send emails from the command line using curl like so:
```
curl "https://api.postmarkapp.com/email" \
  -X POST \
  -H "Accept: application/json" \
  -H "Content-Type: application/json" \
  -H "X-Postmark-Server-Token: <token>" \
  -d '{
        "From": "test@example.com",
        "To": "test@example.com",
        "Subject": "Hello from Postmark",
        "HtmlBody": "<strong>Hello</strong> dear Postmark user.",
        "MessageStream": "outbound"
      }'
```
* Note: the email address in the "From" field has to be one of the approved signatures in Postmark to send the email.

#### `wiremock::MockServer`
* `MockServer::start` asks the operating system for a random available port and spins up the server on a background thread, ready to listen for incoming requests.
* We can then use the `uri` method to get the `base_url` for our `EmailClient`.

#### `wiremock::Mock`
* Out of the box, `wiremock::MockServer` returns "404 Not Found" to all incoming requests.
* We can instruct the mock server to behave differently by mounting a `Mock`.
* When `wiremock::MockServer` receives a request, it iterates over all the mounted mocks to check if the request matches their conditions.
* The matching conditions for a mock are specified using `Mock::given`.
  * Specifying `any()` there would match all incoming requests.
* When an incoming request matches the conditions of a mounted mock, `wiremock::MockServer` returns a response following what was specified in `respond_with`.
* We then mount this `Mock` on our `mock_server`.

#### The Intent Of A Test Should Be Clear
* Using random data generated by `fake` conveys a specific message: do not pay attention to these inputs, their values do not influence the outcome of the test, that’s why they are random.

#### Mock expectations
* Expectations are verified when `MockServer` goes out of scope - at the end of our test function.
* Before shutting down, MockServer will iterate over all the mounted mocks and check if their expectations have been verified.
* If the verification step fails, it will trigger a panic (and fail the test).
* `expect(1)` tells the mock server that it should receive exactly one request that matches this request.

### First Sketch Of `EmailClient::send_email`
* To send an email, we need:
  1. a `POST` request to the `/email` endpoint
  2. a JSON body with fields (PascalCased) that map closely to the arguments of `send_email`
  3. an authorization header, `X-Postmark-Server-Token` from Postmark.

#### `reqwest::Client::post`

#### JSON body
* `reqwest`'s JSON feature does the work to set the `request_body` as the JSON body and also sets the `Content-Type` header to `application/json`.

### Tightening Our Happy Path Test

##### Headers, Path And Method
* We can chain `and` to continue adding expectations for `Mock`, like the `header`, `path`, `method`, etc.
* We will also replace `any()` with the actual header we want to test for

##### Body
* It would be enough to check that the body is valid JSON and it contains the set of field names shown in Postmark’s example.
* Since there isn't an out-of-the-box matcher that suits our needs - we will implement our own: `SendEmailBodyMatcher` where we get the incoming request as input and we need to return a `boolean` value as output whether the mock matched or not.
* To deserialize the request body as JSON - we will add `serde-json` to the list of our development dependencies.
* We need to add `#[serde(rename_all = "PascalCase")]` to our `SendEmailRequest` struct so that we can meet the casing requirement.

#### Refactoring: Avoid Unnecessary Memory Allocations
* For each field of `SendEmailRequest`, we are making copies of String. We can use a string slice (`&str`) to reference existing data.
* A string slice is a just pointer to a memory buffer owned by somebody else.
* To store a reference in a struct we need to add a lifetime parameter: it keeps track of how long those references are valid for - it’s the compiler’s job to make sure that references do not stay around longer than the memory buffer they point to.

### Dealing With Failures
* Two scenarios are:
  1. non-success status codes (e.g. 4xx, 5xx, etc.)
  2. slow responses

#### Error Status Codes
* `Reqwest`'s `send` returns an `Ok` as long as it gets a valid response from the server - no matter the status code.
* So, we need to use `error_for_status()` in `send_email()` to turn a response into an error if the server returned an error so that our `send_email_fails_if_the_server_returns_500` test passes.

#### Timeouts
* We can instruct our mock server to wait a configurable amount of time before sending a response back using `set_delay()`.
* We are not hanging up on the server, so the connection is busy: every time we need to send an email we will have to open a new connection.
  * If the server does not recover fast enough, and we do not close any of the open connections, we might end up with accumulating several "hanging" requests socket exhaustion/performance degradation.
* As a rule of thumb: every time you are performing an IO operation, always set a timeout! But what the timeout should be is an art:
  * too low and you might overwhelm the server with retried requests
  * too high and you risk again to see degradation on the client side
* `reqwest` gives us two options & we will choose the first one.
  1. we can either add a default timeout on the `Client` itself, which applies to all outgoing requests
  2. we can specify a per-request timeout

#### Refactoring: Test Helpers
* We add functions to re-use duplicated code in our tests.

#### Refactoring: Fail fast
* The timeout for our HTTP client is currently hard-coded to 10 seconds, which means running that test takes 10 seconds -- a long time.
* We will make the timeout configurable to keep our test suite responsive.

## Skeleton And Principles For A Maintainable Test Suite

### Why Do We Write Tests?
* Tests mitigate risks, catch bugs in CI, and act as documentation.

### Why Don't We Write Tests?
* Good tests build technical leverage, but writing tests takes time. 

### Test Code Is Still Code
* As the project evolves, there is more friction to add tests - it gets progressively more cumbersome to write new tests.

### Our Test Suite
* All our integration tests live within a single file, tests/health_check.rs.

### Test Discovery
* Our main goal in this refactoring is discoverability:
  * given an application endpoint it should be easy to find the corresponding integration tests within the tests folder
  * when writing a test, it should be easy to find the relevant test helper functions

### One Test File, One Crate
* The `tests` folder is somewhat special - cargo knows to look into it searching for integration tests.
* Each file within the `tests` folder gets compiled as its own crate and is its own executable.
* If we look in `target/debug/deps`, we will see 1 `health_check-*.d` file -- which is our integration tests as confirmed if we run it like `./target/debug/deps/health_check-*`

### Sharing Test Helpers
* If each integration test file is its own executable, how do we share test helpers functions?
* We have 2 options:
  1. define a stand-alone module - e.g. `tests/helpers/mod.rs` to add common functions and then refer to `helpers` in your test files.
  2. take full advantage of that each file under tests is its own executable - we can create sub-modules scoped to a single test executable.
* The first approach will lead to "function is never used warnings"
  * The issue is that `helpers` is bundled as a sub-module, it is not invoked as a third-party crate: `cargo` compiles each test executable in isolation and warns us if, for a specific test file, one or more public functions in helpers have never been invoked. This is bound to happen as your test suite grows - not all test files will use all your helper methods.
* With the 2nd approach, we can add each test file separately with in `tests/api/` with `main.rs` and `helpers.rs` files.
* Each executable is compiled in parallel, the linking phase is instead entirely sequential. So, bundling all your test cases in a single executable like us reduces the time spent compiling your test suite in CI.

### Sharing Startup Logic
* `spawn_app` in tests and our application's `main` looks very similar & whenever we change something in `main`, we need to make the same change in `spawn_app` too.
* This duplication also means that our application's `main` is never tested.

#### Extracting Our Startup Code
* We moved some code from `main` to `build()` in startup.rs
* In `spawn_app`, we have the following phases:
  1. Execute test-specific setup (i.e. initialize a `tracing` subscriber)
  2. Randomize the configuration to ensure tests do not interfere with each other (i.e. a different logical database for each test case)
  3. Initialize external resources (e.g. create and migrate the database)
  4. Build the application
  5. Launch the application as a background task and return a set of resources to interact with it
* We will add an `Application` struct and re-arrange things around to do the same but re-use code.

### Build An API Client
* All of our integration tests are black-box: we launch our application at the beginning of each test and interact with it using an HTTP client (i.e. `reqwest`).
* As we write tests, we necessarily end up implementing a client for our API.
* It gives us a prime opportunity to see what it feels like to interact with the API as a user.
* We can add a function to `TestApp` to share some of the duplicated code in tests/api/subscriptions.rs.

## Refocus
* Every time a user wants to subscribe, they fire a `POST /subscriptions` request
* Our request handler will:
  1. add their details to our database in the `subscriptions` table, with `status` equal to `pending_confirmation`
  2. generate a unique `subscription_token`
  3. store `subscription_token` in our database against their `id` in a `subscription_tokens` table;
  4. send an email to the new subscriber containing a link structured as `https://<api-domain>/subscriptions/confirm?token=<subscription_token>`
  5. return a 200 OK.

* Once they click on the link, a browser tab will open to send a `GET /subscriptions/confirm` endpoint request.
* Our request handler will:
  1. retrieve `subscription_token` from the query parameters
  2. retrieve the subscriber `id` associated with `subscription_token` from the `subscription_tokens` table
  3. update the subscriber `status` from `pending_confirmation` to `active` in the `subscriptions` table
  4. return a 200 OK.

* There are a few other possible designs (e.g. use a JWT instead of a unique token) and we have a few corner cases to handle (e.g. what happens if they click on the link twice or if they try to subscribe twice?).
* We need to find an implementation route that can be rolled out with **zero downtime**.

## Zero Downtime Deployments

### Reliability
* There is no silver bullet to build a highly available solution: it requires work from the application layer all the way down to the infrastructure layer.
* One thing is certain, though: if you want to operate a highly available service, you should master **zero downtime deployments** - users should be able to use the service before, during and after the rollout of a new version of the application to production.
* This is even more important if you are practicing continuous deployment: you cannot release multiple times a day if every release triggers a small outage.

### Deployment Strategies

#### Naive Deployment
* THe "naive” approach looks like:
  * Version A of our service is running in production and we want to roll out version B
    * We switch off all instances of version A running the cluster
    * We spin up new instances of our application running version B
    * We start serving traffic using version B
  * There is a non-zero amount of time where there is no application running in the cluster able to serve user traffic - we are experiencing downtime.

#### Load Balancers
* We have multiple copies of our application running behind a **load balancer**.
  ![Load Balancer](/assets/images/load_balancer.png)
* Even if we have 1 replica of our application running, there is a load balancer between users and the application. Deployments are still performed using a rolling update strategy.
* Each replica of our application is registered with the load balancer as a **backend**.
* Every time somebody sends a request to our API, they hit our load balancer which is then in charge of choosing one of the available backends to fulfill the incoming request.
* Load balancers usually support adding (and removing) backends **dynamically**.
* This enables a few interesting patterns described below.

##### Horizontal Scaling
* We add more capacity when experiencing a spike in traffic by spinning up more replicas of our application.
* It helps spread the load until the work expected of a single instance becomes manageable.

##### Health Checks
* We can ask the load balancer to keep an eye on the **health** of the registered backends.
* Health checking can be:
  1. Passive: the load balancer looks at the distribution of status codes/latency for each backend to determine if they are healthy or not.
  2. Active: the load balancer is configured to send a health check request to each backend on a schedule. If a backend fails to respond with a success code for a set time period, it is marked as unhealthy and removed.
* This is a critical capability to achieve **self-healing** in a cloud-native environment: the platform can detect if an application is not behaving as expected and automatically remove it from the list of available backends to mitigate or nullify the impact on users.

#### Rolling Update Deployments
* We can leverage our load balancer to perform zero downtime deployments.
* Rolling updates:
  * Assume we have three replicas of version A of our application registered as backends for our load balancer.
  * We want to deploy version B.
  * We start by spinning up one replica of version B of our application.
  * When the application is ready to serve traffic (i.e. a few health check requests have succeeded) we register it as a backend with our load balancer.
  * If all is well, we switch off one of the replicas running version A.
  * We continue this until all instances of version A have been replaced by version B.

#### Digital Ocean App Platform
* Digital Ocean boasts zero downtime deployment out-of-the-box, but without details on how it is achieved, however experiments have shown they are using rolling updates.

## Database Migrations
* To ensure high availability in a fault-prone environment, cloud-native applications are **stateless** - they delegate all persistence concerns to external systems (i.e. databases).
* That’s why load balancing works: all backends are talking to the same database to query and manipulate the same **state**.
* The database can be thought of as a single gigantic global variable. Continuously accessed and mutated by all replicas of our application.

### Deployments And Migrations
* During a rolling update deployment, the old and the new version of the application are both serving live traffic, side by side.
* So, the old and the new version of the application are using the **same database at the same time**.
* To avoid downtime, we need a database schema that is understood by both versions.
* This is not an issue for most of our deployments, but it is a serious constraint when we need to evolve the schema.
* To move forward with the implementation strategy for confirmation emails, we need to evolve our database schema as:
  1. add a new table `subscriptions_tokens`
  2. add a new mandatory column, `status`, to the existing `subscription` table.

* Possible scenarios showing we cannot possibly deploy confirmation emails all at once without incurring downtime:
  * We could first migrate the database and then deploy the new version.
    * This implies that the current version is running against the migrated database for some time: our current implementation of `POST /subscriptions` does not know about `status` and it tries to insert new rows into  `subscriptions` without populating it.
    * Given that `status` is a mandatory field, all inserts would fail - we would not be able to accept new subscribers until the new version of the application is deployed.
  * We could first deploy the new version and then migrate the database.
    * We get the opposite scenario.
    * When `POST /subscriptions` is called, it tries to insert a row into `subscriptions` with a `status` field that does not exist - all inserts fail and we cannot accept new subscribers until the database is migrated.

### Multi-step Migrations
* A big bang release won’t work - we need to get there in multiple, smaller steps. We'll keep the application code stable and migrate the database.

### A New Mandatory Column
* We look at the `status` column

#### Step 1: Add As Optional
1. We generate and run a new migration script to add `status` as an optional field to our table.
2. We test the migration and our tests on our local database to confirm all is good before running the migration on our production database.

#### Step 2: Start Using The New Column
1. We can start using `status`, every time a new subscriber is inserted, we set `status` to `confirmed` by changing our `INSERT` query in our application.
2. Again, we run the tests and then deploy the new version of the application to production.

#### Step 3: Backfill And Mark As `NOT NULL`
* The latest version of the application ensures that status is populated for all new subscribers.
* To mark `status` as `NOT NULL` we just need to backfill the value for historical records: we’ll then be free to alter the column.
1. We generate a new migration script to mark `status` as `NOT NULL` and backfill `status` for historical entries with `confirmed`.
2. Migrate our local database, run tests, and deploy to production database.

### A New Table
* This is easier.
* We add the new table in a migration while the application keeps ignoring it.
* We can then deploy a new version of the application that uses it to enable confirmation emails.
1. The migration script will add a table with the necessary columns.
2. We migrate our local database, run tests, and deploy to production.

**Note: We add a migration script with `sqlx migrate <script-name>`**

## Sending A Confirmation Email

### A Static Email
* We will test that `POST /subscriptions` is sending out an email without focusing on the body of the email for now.

#### Red Test
* We need to spin up a mock server to stand in for Postmark’s API and intercept outgoing requests.
* We add a `subscribe_sends_a_confirmation_email_for_valid_data` test.

#### Green Test
* We add the necessary changes to make the test green.
  * This includes adding a mock for `subscribe_returns_a_200_for_valid_form_data` which would now fail since it tries to send an email.

### A Static Confirmation Link
* We will scan the body of the email to retrieve a confirmation link.

#### Red Test
* For now, we will just test that there is something that is a link.
* We can use `received_requests` on `MockServer` to intercept all the requests (as a vector) intercepted by the server if the request recording is enabled (the default).
* To extract links out of the email body, we will use `linkify` and make sure the 1 link exists in the html body and 1 in the text body and that both links are identical.

#### Green Test
* We add a dummy confirmation link to the email to pass the test.

### Pending Confirmation
* We will set a new subscriber's status to `pending_confirmation` instead of `confirmed`.

#### Red Test
* We will look at the `status` of a new subscriber.

#### Green Test
* Instead of inserting `confirmed`, we will insert `pending_confirmation` when a new subscriber signs up.

### Skeleton of `GET /subscriptions/confirm`
* We want to build up the skeleton of the endpoint - we need to register the handler against the path in src/startup.rs and reject incoming requests without the required query parameter, `subscription_token`.

#### Red Test
* We will add a new module for tests relating to the confirmation callback.

#### Green Test
* We want to make sure that there is a `subscription_token` query parameter: we can use `actix-web`'s extractor: `Query`.
* The `Parameters` struct defines all the query parameters that we *expect* to see in the incoming request.
* It needs to implement `serde::Deserialize` to enable` actix-web` to build it from the incoming request path.
* A function parameter of type `web::Query<Parameter>` is enough to instruct `actix-web` to only call the handler if the extraction was successful
* If the extraction failed a 400 Bad Request is automatically returned.

### Connecting The Dots

#### Red Test
* We will behave like a user: we will call `POST /subscriptions`, extract the confirmation link from the outgoing email request, and then call it to confirm our subscription expecting a 200 OK.

#### Green Test
* The domain & protocol will vary based on the environment that we're running in. http://127.0.0.1 for our tests and another with HTTPS in production, so we can add a new field in `ApplicationSettings` to make this configurable.
* We also need to update `spec.yaml` to add the `APP_URL` and get the app identifier from `doctl apps list --format ID` and run `doctl apps update $APP_ID --spec spec.yaml` to apply these changes to DigitalOcean.
* Our test will still fail:
  * `reqwest::Client` will fail to establish a connection since `port` will be `None` when sending requests to `http:127.0.0.1/subscriptions/confirm` without specifying the port for our test server.
  * This is non-issue for production workloads where the DNS domain is enough, so we’ll just patch it in the test by storing a port in `TestApp`.
  * We will also add a `subscription_token` query parameter to the confirmation link.

#### Refactor
* We will add a `get_confirmation_links` function to do all the work in extracting the links from the body, setting the port in the confirmation link, etc.

### Subscription Tokens

#### Red Test
* We will add another test to confirm that clicking on a confirmation link would make a subscriber's `status` confirmed.

#### Green Test
* We will use a cryptographically secure pseudorandom number generator - a `CSPRNG` as our subscription token using `rand` and store it with the corresponding subscriber's id.
* On the `GET /subscriptions/confirm` side, we need to:
  1. get a reference to the database pool,
  2. retrieve the subscriber id associated with the token (if one exists)
  3. change the subscriber status to `confirmed`
 
## Database Transactions

### All Or Nothing
* `POST /subscriptions` handler has grown in complexity - we are now performing two `INSERT` queries against our Postgres database: one to store the details of the new subscriber, one to store the newly generated subscription token.
* If our application crashes between those 2 queries, our database could be left in a bad state.
* Relational databases (and a few others) provide a mechanism to mitigate this issue: **transactions**.
* Transactions are a way to group together related operations in a single **unit of work**.
* The database guarantees that all operations within a transaction will succeed or fail together.

### Transactions In Postgres
* `BEGIN` is used to start a transaction and `COMMIT` is used to mark its end.
* If any of the queries within a transaction fails the database **rolls back**: all changes performed by previous queries are reverted, the operation is aborted.
* Transactions also hide the effect of uncommitted changes from other queries that might be running, concurrently, against the same tables.

### Transactions In Sqlx
* By calling `begin` on our pool we acquire a connection from the pool and kick off a transaction.
* `begin`, if successful, returns a `Transaction` struct.
* A mutable reference to a `Transaction` dereferences to `sqlx`’s `Connection`, so it can be used to run queries. All queries run using a `Transaction` as executor become part of the transaction.
* We can also rollback a transaction manually.
* If `commit` or `rollback` have not been called before the `Transaction` object goes out of scope (i.e. `Drop` is invoked), a `rollback` command is queued to be executed as soon as an opportunity arises.

# Ch 8 - Error Handling

## What Is The Purpose Of Errors?
* `execute` to run sql queries is a fallible operation: issues with talking to the database, violating uniqueness, etc.

### Internal Errors
#### Enable The Caller To React
* The caller of fallible functions most likely wants to be informed if a failure occurs -** they need to react accordingly**, e.g. retry the query or propagate the failure upstream using `?`.
* When the return type is a `Result<Success, Error>` (e.g. in `execute`), the compiler forces the caller to handle both scenarios - success and failure.
* If our only goal was to communicate to the caller that an error happened, we could use a simpler definition for `Result`: `ResultSignal<Success>`:
  * ```Rust
      pub enum ResultSignal<Success> {
        Ok(Success)
        Err
      }
    ```
* There would be no need for a generic `Error` type - we could just check that execute returned the `Err` variant:
  * ```Rust
      let outcome = transaction.execute(query).await;
      if outcome == ResultSignal::Err {
        // Do something if it failed
      }
    ```
* This works if there is only one failure mode. Truth is, operations can fail in multiple ways and we might want to react differently depending on what happened.
  * `sqlx::Error` is implemented as an enum to allow users to match on the returned error and behave differently depending on the underlying failure mode.

#### Help An Operator To Troubleshoot
* Even if an operation has a single failure mode, while `Err(())` might have been enough for the caller to determine what to do - e.g. return a 500 Internal Server Error to the user, we errors to carry enough context about the failure to produce a report with enough details for an operator (e.g. the developer) to troubleshoot the issue.

### Errors At The Edge
#### Help A User To Troubleshoot
* For internal errors, we don't necessarily need to tell the user why an operation failed, but conveying that information to them is helpful in other cases.
* `SubscriberEmail`'s `parse` returns a semi-helpful message but it is not displayed to the user since `subscribe()` just returns a `BadRequest`. This is a poor error, since the user can't see why it was a `BadRequest` or how can they adapt their behavior.

## Error Reporting For Operators
* To see if we are doing a good job reporting errors, we add a new test `subscribe_fails_if_there_is_a_fatal_database_error` and look at the logs.
* `sqlx` logs are spammy so so we can reduce noise by:
  * ```
      export RUST_LOG="sqlx=error,info"
      export TEST_LOG=true
      cargo t subscribe_fails_if_there_is_a_fatal_database_error | bunyan
    ```
* We will see an error message in the logs: `"Failed to execute query: column "subscription_token" of relation "subscription_tokens" does not exist"`, which could tell us what the issue is, but it would be nice if the HTTP error that returned 500 can include details on the underlying issue in the `exception.details` and `exception.message`.

### Keeping Track Of The Error Root Cause
* The "Failed to execute query:" part of the error log came from the `execute` call in `store_subscription_token`.
* We propagate the error upwards using the `?` operator, but the chain breaks in `subscribe` - we discard the error we received from `store_subscription_token` and return a bare 500 response.
* `HttpResponse::InternalServerError().finish()` is the only thing that `actix_web` and `tracing_actix_web::TracingLogger` get to access when they are about to emit their respective log records.
  * The error does not contain any context about the **underlying root cause**, therefore the log records are equally useless.
* To fix it, we can implement `ResponseError` for our errors like `impl ResponseError for sqlx::Error {}`
* But the compiler gives us an error: `sqlx::Error is not defined in the current crate`.
  * This is Rust's orphan rule: it is forbidden to implement a foreign trait for a foreign type, where the foreign type stands for "from another crate".
  * This restriction is meant to preserve coherence: imagine if you added a dependency that defined its own implementation of ResponseError for `sqlx::Error` - which one should the compiler use when the trait methods are invoked?
* Orphan rule aside, it would still be a mistake for us to implement `ResponseError` for `sqlx::Error`.
  * We want to return a "500 Internal Server Error" when we run into a `sqlx::Error` while trying to persist a subscriber token.
  * In another circumstance we might wish to handle a `sqlx::Error` differently.
  * So, we wrap the underlying error `sqlx::Error` into `StoreTokenError` and implement `ResponseError` on that.
  * We would now need to implement `Display` and `Debug` traits for `StoreTokenError`.
    * `Display` should return a programmer-facing representation, as faithful as possible to the underlying type structure, to help with debugging.
    * `Debug` should return a user-facing representation of the underlying type.
* We can now use this in the request handler:
  * We will now have to wrap (early) returns `subscribe` in `Ok(...)` as well
  * The `?` operator transparently invokes the `Into` trait on our behalf - we don't need an explicit `map_err` anymore when we call `store_subscription_token`.
  * Now, when we run the `subscribe_fails_if_there_is_a_fatal_database_error` test with sqlx logs, we will see the exception message contains: "A database error was encountered while trying to store a subscription token."

### The `Error` Trait
* What is the point of implementing the `Error` trait at all for our error type?
  * The `Error` trait is a way to semantically mark our type as being an error. It helps a reader of our codebase to immediately spot its purpose.
  * It is also a way for the Rust community to standardize on the minimum requirements for a good error:
    * it should provide different representations (`Debug` and `Display`), tuned to different audiences
    * it should be possible to look at the underlying cause of the error, if any (`source`)

#### Trait Objects
* Trait objects, just like generic type parameters, are a way to achieve polymorphism in Rust: invoke different implementations of the same interface.
* Generic types are resolved at compile-time (static dispatch), trait objects incur a runtime cost (dynamic dispatch).

#### `Error::source`
* We will implement `Error` for `StoreTokenError`.
* `source` is useful when writing code that needs to handle a variety of errors: it provides a structured way to navigate the error chain without having to know anything about the specific error type you are working with.
* If we look at our log record, the causal relationship between `StoreTokenError` and `sqlx::Error` is somewhat implicit - we infer one is the cause of the other because *it is a part of it*.
  * To make it more explicit, we can implement `Debug` on `StoreTokenError`.
* Using `source` we can write a function that provides a similar representation for any type that implements `Error`.
  * `error_chain_fmt` iterates over the whole chain of errors that led to the failure we are trying to print.
  * We can then change our implementation of `Debug` for `StoreTokenError` to use it.

## Errors For Control Flow

### Layering
* We got good logs, but had to implement a trait from our web framework (`ResponseError`) for an error type returned by an operation that is unaware of REST or the HTTP protocol, `store_token`.
* We could be calling store_token from a different entry-point (e.g. a CLI) - nothing should have to change in its implementation.
* Even assuming we are only ever going to be calling `store_token` in the context of a REST API, we might add other endpoints that rely on that routine - they might not want to return a 500 when it fails.
* Choosing the appropriate HTTP status code when an error occurs is a concern of the request handler, it should not leak elsewhere, so we delete `ResponseError`.
* To enforce a proper separation of concerns we need to introduce another error type, `SubscribeError`.
  * We will use it as failure variant for `subscribe` and it will own the HTTP-related logic (`ResponseError`’s implementation).
* This will result in an avalanche of `?` couldn't convert the error to `SubscribeError` - so we need to implement conversions from the error types returned by our functions and `SubscribeError`.

### Modeling Errors as Enums
* An enum is the most common approach to work around this issue: a variant for each error type we need to deal with.
  * We can then leverage the `?` operator in our handler by providing a `From` implementation for each of wrapped error types.
  * And we should be able to clean up all our request handlers by removing all the `match/if fallible_function().is_err()` lines and use `?` instead.
  * Code compiles but the `subscriptions::subscribe_returns_a_400_when_fields_are_present_but_invalid` test fails since we are using the default implementation of `ResponseError` that always returns 500.
  * We can fix this by providing an implementation for `ResponseError` to use a `match` statement for control flow - we behave differently depending on the failure scenario we are dealing with.

### The Error Type Is Not Enough
* Looking at the `subscribe_fails_if_there_is_a_fatal_database_error` test's info logs, we can see that it is using `StoreTokenError` in the `exception.details`.
* The `exception.message`, however, always says "Failed to create a new subscriber" regardless of what the failure mode was.
  * So, we will refine our `Debug` and `Display` implementations.
* There is a problem when we try to implement `Display` - the same `DatabaseError` variant is used for errors encountered when:
  * acquiring a new Postgres connection from the pool
  * inserting a subscriber in the subscribers table
  * committing the SQL transaction
* When implementing `Display` for `SubscribeError` we have no way to distinguish which of those three cases we are dealing with - the underlying error type is not enough.
  * So, we will breakdown `DatabaseError` into 3 new variants within `SubscribeError`.
* The code compiles and `exception.message` is helpful again.

### Removing The Boilerplate With `thiserror`
* All the `source` and `From` implementations are a lot of boilerplate but we can use a crate `thiserror` to generate all the boilerplate using a macro.
* With using the macro, we are able to get rid of the `Display`, `From`, and `Error` implementations.
* `thiserror` is a procedural macro and receives, at compile-time, the definition of `SubscriberError` as input and returns new Rust code, which is then compiled into the final binary.
* Within the context of `#[derive(thiserror::Error)]`, we get access to other attributes:
  * `#[error(/* */)]` defines the `Display` representation of the enum variant it is applied to.
    * E.g. `Display` will return "Failed to send a confirmation email" when invoked on an instance of `SubscribeError::SendEmailError`.
    * You can interpolate values in the final representation - e.g. the `{0}` in `#[error("{0}")]` on top of `ValidationError` is referring to the wrapped `String` field, similar to the syntax to access fields on tuple structs (i.e. `self.0`).
    * `#[source]` is used to denote what should be returned as root cause in `Error::source`.
    * `#[from]` automatically derives an implementation of `From` for the type it has been applied to into the top-level error type (e.g.`impl From<StoreTokenError> for SubscribeError {/* */}`).
    * A field annotated with `#[from]` is also used as error source, saving us from having to use two annotations on the same field.
**We are not using either `#[from]` or `#[source]` for the `ValidationError` variant. That is because `String` does not implement the `Error` trait, therefore it cannot be returned in `Error::source` - the same limitation we encountered before when implementing `Error::source` manually, which led us to return `None` in the `ValidationError` case.**

## Avoid "Ball Of Mud" Error Enums
* In `SubscribeError` we are using enum variants for two purposes:
  * Determine the response that should be returned to the caller of our API(`ResponseError`)
  * Provide relevant diagnostic (`Error::source`,`Debug`,`Display`)
* `SubscribeError` is exposing a lot of the implementation details of `subscribe`: we have a variant for every fallible function call we make in the request handler. This strategy does not scale very well.
* The caller of `subscribe` does not understand the intricacies of the subscription flow: they don’t know enough about the domain to behave differently for a `SendEmailError` compared to a `TransactionCommitError` (by design). `subscribe` should return an error type that speaks at the right level of abstraction.
* We can map `ValidationError` to a `400 Bad Request` and map `UnexpectedError` to `500 Internal Server Error` without exposing the implementation details.
* We can use `Box<dyn std::error:Error>`

### Using `anyhow` As Opaque Error Type
* `anyhow::Error` is a wrapper around a dynamic error type. 
* `anyhow::Error` works a lot like `Box<dyn std::error::Error>`, but with these differences:
* `anyhow::Error`requires that the error is `Send`, `Sync`, and `'static`
* `anyhow::Error` guarantees that a backtrace is available even if the underlying error type does not provide one.
* `anyhow::Error` is represented as a narrow pointer — exactly one word in size instead of two.
* The `context` method:
  * converts the error returned by our methods into an `anyhow::Error`
  * enriches it with additional context around the intentions of the caller

### `anyhow` Or `thiserror`
* A Rust myth: `anyhow` is for applications, `thiserror` is for libraries.
* Do you expect the caller to behave differently based on the failure mode they encountered?
  * Use an error enumeration, empower them to match on the different variants. Bring in `thiserror` to write less boilerplate.
* Do you expect the caller to just give up when a failure occurs? Is their main concern reporting the error to an operator or a user?
  * Use an opaque error, do not give the caller programmatic access to the error inner details. Use `anyhow` or `eyre` if you find their API convenient.
  
## Who Should Log Errors?
* As a rule of thumb, errors should be logged when they are handled.
* If your function is propagating the error upstream (e.g. using the `?` operator), it should not log the error. It can, if it makes sense, add more context to it.
* If the error is propagated all the way up to the request handler, delegate logging to a dedicated middleware - `tracing_actix_web::TracingLogger` in our case.

# Ch 9 - Naive Newsletter Delivery

## User Stories Are Not Set In Stone
* Now that we have confirmed and unconfirmed subscribers, the user story will be modified:
  * As the blog author, I want to send an email to all my confirmed subscribers, so that I can notify them when new content is published.

## Do Not Spam Unconfirmed Subscribers
* We will add tests on the fact that no email is sent out through Postmark when we send a newsletter where all the subscribers are unconfirmed.

### Set Up State Using The Public API
* Each test spins up a brand-new application running on top of an empty database.
* We stay true to the black-box approach, we drive the application state by calling its public API when making a `POST` call to the `/subscriptions` endpoint.

### Scoped Mocks
* `POST /subscriptions` will send a confirmation email out - so we must make sure that our Postmark test server is ready to handle the incoming request by setting up the appropriate `Mock`.
* To make sure the 2 mocks don't end up stepping on each other's toes (since the matching logic overlaps what we have in the test function body), we use a **scoped** Mock.
* `MockGuard` has a custom `Drop` implementation: when it goes out of scope, `wiremock` instructs the underlying `MockServer` to stop honoring the specified mock behavior. So, we stop returning `200` to `POST /email` at the end of `create_unconfirmed_subscriber`.
* When `MockGuard` is dropped, we check the expectations on the scoped mock are verified too.

### Green Test
* We will turn the test green with a dummy implementation of `publish_newsletter`.

## All Confirmed Subscribers Receive New Issues

### Composing Test Helpers
* We'll write another integration test for the happy case: a confirmed subscriber would receive the new issue of the newsletter via email.
  * We'll add another function for creating a confirmed subscriber.

## Implementation Strategy
* Our naive approach would be:
  * Retrieve the newsletter issue details from the body of the incoming API call
  * Fetch all the `confirmed` subscribers from the database
  * Iterate through the list
    * Get the subscriber email
    * Send an email via Postmark

## Body Schema
* What serialization format are we using?
  * For `POST /subscriptions`, given that we were dealing with HTML forms, we used `application/x-www-form-urlencoded` as `Content-Type`.
  * For `POST /newsletters` we are not tied to a form embedded in a webpage: we will use JSON,a common choice when building REST APIs.

### Test Invalid Inputs
* We'll add a test to make sure the newsletter endpoint returns a 400 if bad data is passed to it.

## Fetch Confirmed Subscribers List
* We use `sqlx::query_as!` instead of `sqlx::query!` in `get_confirmed_subscribers` since `query_as!` maps the retrieved rows to the type specified as its first argument saving us a bunch of boilerplate.

## Send Newsletter Email
* We can leverage the email client that is already part of the app state and we can extract it using `web::Data` to send the emails to all the confirmed subscribers.

### `context` Vs `with_context`
* We are using a new method, `with_context`.
  * We used `context` in previous chapters to convert the error variant of `Result` into `anyhow::Error` while enriching it with contextual information.
  * There is one key difference between the two: `with_context` is **lazy**.
    * It takes a closure as argument and the closure is only called in case of an error
    * If the context you are adding is static - e.g. context("Oh no!")-they are equivalent.
    * If the context you are adding has a runtime cost, use `with_context` - you avoid paying for the error path when the fallible operation succeeds.
    * Example: `format!` allocates memory on the heap to store its output string; using `context`, we would be allocating that string every time we send an email out.
    * Using `with_context`, instead, we only invoke `format!` if email delivery fails.

## Validation of Stored Data
* Is `SubscriberEmail::parse(row.email).unwrap()` a good idea for stored data?
  * The emails of all new subscribers go through the validation logic in `SubscriberEmail::parse` - it was a big focus topic for us in Chapter 6.
  * One might argue that all the emails stored in our database are valid - there is no need to account validation failures here. It is safe to just unwrap them all, knowing it will never panic.
  * Data stored in our Postgres instance creates a **temporal coupling between old and new versions of our application**.
  * The emails we are retrieving from our database were marked as valid by a **previous** version of our application. The **current** version might disagree.
    * E.g. We might discover that our email validation logic is too lenient - some invalid emails are slipping through the cracks, leading to issues when attempting to deliver newsletters.
    * We implement a stricter validation routine, deploy the patched version and, suddenly, email delivery does not work at all. `get_confirmed_subscribers` panics when processing stored emails that were previously considered valid, but no longer are.
  * What should we do? Skip validation entirely when retrieving data from the database? There is no one-size-fits-all answer.
  * Sometimes it is unacceptable to process invalid records - the routine should fail and an operator must intervene to rectify the corrupt records.
  * Sometimes we need to process all historical records (e.g. analytics) and we should make minimal assumptions about the data - `String` is our safest bet.
  * In our case, we meet halfway: we can skip invalid emails when fetching the list of recipients for our next newsletter issue.
    * We will emit a warning for every invalid address we find, allowing an operator to identify the issue and correct the stored records if needed.
* `filter_map` returns a new iterator containing only the items for which our closure returned a `Some` variant.

### Responsibility Boundaries
* Is `get_confirmed_subscriber` the most appropriate location to choose if we should skip or abort when encountering an invalid email address?
* It feels like a business-level decision that would be better placed in `publish_newsletter`, the driving routine of our delivery workflow.
* `get_confirmed_subscriber` should simply act as an adapter between our storage layer and our domain layer.
  * It deals with the database-specific bits (i.e. the query) and the mapping logic, but it delegates to the caller the decision on what to do if the mapping or the query fail.

## Limitations Of The Naive Approach
* Our approach so far has the following limitations:
  1. Security: Our `POST /newsletters` endpoint is unprotected - anyone can fire a request to it and broadcast to our entire audience, unchecked.
  2. Only One Shot: As soon you hit `POST /newsletters` your content goes out to your entire mailing list. No chance to edit or review it in draft mode, etc.
  3. Performance: We are sending emails out one at a time. We wait for the current one to be dispatched successfully before moving on to the next in line.
  4. Fault Tolerance: If we fail to dispatch one email we bubble up the error using `?` and return a `500 Internal Server Error` to the caller. The remaining emails are never sent, nor we retry to dispatch the failed one.
  5. Retry Safety: Many things can go wrong when communicating over the network. What should a consumer of our API do if they experience a timeout or a `500 Internal Server Error`? They cannot retry - they risk sending the newsletter issue twice to the entire mailing list.

# Ch 10 - Securing Our API
* To prevent anyone from hitting our API and sending a newsletter to our entire newsletter mailing list, we need to secure it.

## Authentication
* Authentication boils down to 3 categories:
  1. Something they know (password, pin, security questions)
  2. Something they have (an authenticator app)
  3. Something they are (fingerprints, Face ID)

### Drawbacks
#### Something They Know
* Passwords must be long, unique, and not reused.

#### Something They Have
* Smartphones and keys can be lost or stolen leaving the user locked out or vulnerable.

#### Something They Are
* Biometrics can't be "rotated"

### Multi-factor Authentication
* We combine at least 2 forms of authentication to overcome most of the weaknesses individual techniques have.

## Password-based Authentication
* We can use the ‘Basic’ Authentication Scheme, a standard defined by the Internet Engineering Task Force (IETF) in RFC 2617.
* The API must look for the Authorization header in the incoming request, structured as `Authorization: Basic <encoded credentials>` where `<encoded credentials>` is the base64-encoding of `{username}:{password}`.
* According to the specification, we need to partition our API into protection spaces or **realms** - resources within the same realm are protected using the same authentication scheme and set of credentials.
  * We only have a single endpoint to protect - `POST /newsletters` so we'll have a single realm, named `publish`.
* The API must reject all requests missing the header or using invalid credentials - the response must use the `401 Unauthorized` and include a special header, `WWW-Authenticate`,containing a **challenge**.
* The challenge is a string explaining to the API caller what type of authentication scheme we expect to see for the relevant realm.
* In our case, using basic authentication, the challenge should be:
  ```
  HTTP/1.1 401 Unauthorized
  WWW-Authenticate: Basic realm="publish"
  ``` 

#### Extracting Credentials
* We extract the encoded credentials from the `Authorization` header in the request, make sure it is `Basic`, decode them, make sure they are valid UTF8, and that both username and password are provided.

### Password Verification - Naive Approach
* We will store the username and password in a new table and compare the incoming password with what we have before we authenticate a user.
* We will generate a test user for every instance of our test application. We have not yet implemented a sign-up flow for newsletter editors, therefore we cannot go for a fully black-box approach.
  * So for now, we will inject the test user details directly into the database when the app is being spawned.

### Password Storage

#### No Need To Store Raw Passwords
* We can use a `cryptographic hash function f` to avoid storing the raw password altogether: when a user signs up, we compute `f(password)` and store it in our database. `password` is discarded.
* When the same user tries to sign in, we compute `f(psw_candidate)` and check that it matches the `f(password)` value we stored during sign-up. The raw password is never persisted.

* Hash functions map strings from the input space to **fixed-length** outputs.
* **Cryptographic** refers to the uniformity property we were just discussing, also known as **avalanche effect**: a tiny difference in inputs leads to outputs so different to the point of looking uncorrelated.
* From MD5, SHA-1, SHA-3, Kangaroo Twelve, we will choose SHA-3 as our hash function.
* On top of the algorithm, we will choose SHA3-224 that uses the SHA-3 algorithm to produce a fixed-sized output of 224 bits.
